{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conala with original data with python embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPU/ZZ3eDG2OlZWtpNJv3tt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/Python-Code-Generation/blob/main/Conala_with_original_data_with_python_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3jYaF2NxaFI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OMTNG2PxhOq",
        "outputId": "2b60a9fc-9a67-4128-b538-dabb5a2d6fc6"
      },
      "source": [
        "! git clone https://github.com/akashe/Python-Code-Generation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Python-Code-Generation'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 82 (delta 39), reused 40 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY-aGuxDxist"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/Python-Code-Generation/\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9tutyvZxkrg"
      },
      "source": [
        "from data_processing import getTokenizer,getData"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTvI-Pb_xl1m",
        "outputId": "553fa597-cbd9-4e58-9d2d-8c6b0b46c2a7"
      },
      "source": [
        "!wget -c \"http://www.phontron.com/download/conala-corpus-v1.1.zip\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-12 15:03:49--  http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52105440 (50M) [application/zip]\n",
            "Saving to: ‘conala-corpus-v1.1.zip’\n",
            "\n",
            "conala-corpus-v1.1. 100%[===================>]  49.69M  30.4MB/s    in 1.6s    \n",
            "\n",
            "2021-03-12 15:03:51 (30.4 MB/s) - ‘conala-corpus-v1.1.zip’ saved [52105440/52105440]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvNkOb54xmQX",
        "outputId": "3a183d25-86b0-4646-c1b6-4d6eb4c6db4c"
      },
      "source": [
        "!unzip -n conala-corpus-v1.1.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  conala-corpus-v1.1.zip\n",
            "   creating: conala-corpus/\n",
            "  inflating: conala-corpus/conala-mined.jsonl  \n",
            "  inflating: conala-corpus/conala-train.json  \n",
            "  inflating: conala-corpus/conala-test.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En3A5BQWxn9P",
        "outputId": "717681dc-a1b8-401f-8249-537ea6e86ca1"
      },
      "source": [
        "!head -20 conala-corpus/conala-test.json"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"intent\": \"How can I send a signal from a python program?\",\n",
            "    \"rewritten_intent\": \"send a signal `signal.SIGUSR1` to the current process\",\n",
            "    \"snippet\": \"os.kill(os.getpid(), signal.SIGUSR1)\",\n",
            "    \"question_id\": 15080500\n",
            "  },\n",
            "  {\n",
            "    \"intent\": \"Decode Hex String in Python 3\",\n",
            "    \"rewritten_intent\": \"decode a hex string '4a4b4c' to UTF-8.\",\n",
            "    \"snippet\": \"bytes.fromhex('4a4b4c').decode('utf-8')\",\n",
            "    \"question_id\": 3283984\n",
            "  },\n",
            "  {\n",
            "    \"intent\": \"check if all elements in a list are identical\",\n",
            "    \"rewritten_intent\": \"check if all elements in list `myList` are identical\",\n",
            "    \"snippet\": \"all(x == myList[0] for x in myList)\",\n",
            "    \"question_id\": 3844801\n",
            "  },\n",
            "  {\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjinuEdDxr4n"
      },
      "source": [
        "questions, answers = [],[]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PpIlzVNxtNI"
      },
      "source": [
        "# loading train json\n",
        "f = open(\"/content/conala-corpus/conala-train.json\",\"r\")\n",
        "train_file = json.load(f)\n",
        "f.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QriUuRU4xuiH"
      },
      "source": [
        "for num,i in enumerate(train_file):\n",
        "  if i['intent'] is not None:\n",
        "    questions.append(i['intent'])\n",
        "    answers.append(i['snippet'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK27P2rsxwNE"
      },
      "source": [
        "# loading train json\n",
        "f = open(\"/content/conala-corpus/conala-test.json\",\"r\")\n",
        "test_file = json.load(f)\n",
        "f.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKWO3SymxxLB"
      },
      "source": [
        "for num,i in enumerate(test_file):\n",
        "  if i['intent'] is not None:\n",
        "    questions.append(i['intent'])\n",
        "    answers.append(i['snippet'])\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq185vXoxyvq"
      },
      "source": [
        "questions_, answers_ = getData(\"/content/Python-Code-Generation/data/english_python_data_pruned.txt\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sLzaWVxxzMe"
      },
      "source": [
        "# Setting max word len\n",
        "max_word_len = 301"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1xBUHYmx0mF",
        "outputId": "8dfb14ab-9cad-47f9-877e-76b054f9b398"
      },
      "source": [
        "# removing examples with len more than max_word_len\n",
        "# Only checking in original data because I checked in CONALA data and answers their dont exceed 301.\n",
        "pruned_questions = []\n",
        "pruned_answers = []\n",
        "for j,i in zip(questions_,answers_):\n",
        "  tokens = getTokenizer(i)\n",
        "  if not len(tokens) > max_word_len:\n",
        "    pruned_answers.append(i)\n",
        "    pruned_questions.append(j)\n",
        "\n",
        "print(len(pruned_answers))\n",
        "answers_ = pruned_answers\n",
        "questions_ = pruned_questions"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in tokenization\n",
            "3058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5sUoxQGx1-a"
      },
      "source": [
        "questions = questions + questions_\n",
        "answers = answers + answers_"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liZsYtBQx3U7",
        "outputId": "23c11677-893e-45cd-f3a7-70c45651f3db"
      },
      "source": [
        "questions[100],answers[100]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Clicking a link using selenium using python',\n",
              " \"driver.find_element_by_xpath('xpath').click()\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB-Z3Yu-x4Yz",
        "outputId": "643819c8-dd06-4ad9-d5b3-6ed0340ce805"
      },
      "source": [
        "print(len(questions),len(answers))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5937 5937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHjZk5zxx5ik"
      },
      "source": [
        "SEED = 1327\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPECgzYox6xt",
        "outputId": "234d1b6a-0ff0-4706-900d-cb1b73005417"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RKCvzMnx7_v"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iT71I83x9FP"
      },
      "source": [
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNLt9Bhfx-vp"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = getTokenizer, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = False, \n",
        "            batch_first = True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0dwLgsxx_Nl",
        "outputId": "dd307449-5830-41a0-e6ec-83ab249d7f51"
      },
      "source": [
        "fields = [('src', SRC), ('trg', TRG)]\n",
        "\n",
        "Examples = [data.Example.fromlist([i,j], fields) for i,j in zip(questions,answers)]\n",
        "Dataset = data.Dataset(Examples, fields)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAX890DdyA6e"
      },
      "source": [
        "train_data,valid_data = Dataset.split(split_ratio=[0.85,0.15])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jz4qsZPyBdw"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fdl1V03yDlw",
        "outputId": "a8e1bd2c-855b-4044-c020-bff2d7d5d534"
      },
      "source": [
        "print(len(SRC.vocab))\n",
        "print(len(TRG.vocab))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1661\n",
            "2666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKq0T5zAyEsH"
      },
      "source": [
        "# Dumps dicts\n",
        "with open(\"/content/SRC_stio_local\",\"wb\") as f:\n",
        "  pickle.dump(SRC.vocab.stoi,f)\n",
        "with open(\"/content/TRG_itos_local\",\"wb\") as f:\n",
        "  pickle.dump(TRG.vocab.itos,f)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkRq2zvqyJCK",
        "outputId": "7c8386b6-717f-4739-9616-a1a29216f7e7"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvDEFNU2yKph"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_key = lambda x: len(x.trg),\n",
        "     device = device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxfoYfe0yLIy"
      },
      "source": [
        "class PositionalEncodingComponent(nn.Module):\n",
        "  '''\n",
        "  Class to encode positional information to tokens.\n",
        "  \n",
        "\n",
        "  '''\n",
        "  def __init__(self,hid_dim,device,dropout=0.2,max_len=5000):\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim%2==0 # If not, it will result error in allocation to positional_encodings[:,1::2] later\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.positional_encodings = torch.zeros(max_len,hid_dim)\n",
        "\n",
        "    pos = torch.arange(0,max_len).unsqueeze(1) # pos : [max_len,1]\n",
        "    div_term  = torch.exp(-torch.arange(0,hid_dim,2)*math.log(10000.0)/hid_dim) # Calculating value of 1/(10000^(2i/hid_dim)) in log space and then exponentiating it\n",
        "    # div_term: [hid_dim//2]\n",
        "\n",
        "    self.positional_encodings[:,0::2] = torch.sin(pos*div_term) # pos*div_term [max_len,hid_dim//2]\n",
        "    self.positional_encodings[:,1::2] = torch.cos(pos*div_term) \n",
        "\n",
        "    self.positional_encodings = self.positional_encodings.unsqueeze(0) # To account for batch_size in inputs\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.positional_encodings[:,:x.size(1)].detach().to(self.device)\n",
        "    return self.dropout(x)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZoZTEBayMS7"
      },
      "source": [
        "class FeedForwardComponent(nn.Module):\n",
        "  '''\n",
        "  Class for pointwise feed forward connections\n",
        "  '''\n",
        "  def __init__(self,hid_dim,pf_dim,dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.fc1 = nn.Linear(hid_dim,pf_dim)\n",
        "    self.fc2 = nn.Linear(pf_dim,hid_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # x : [batch_size,seq_len,hid_dim]\n",
        "    x = self.dropout(torch.relu(self.fc1(x)))\n",
        "\n",
        "    # x : [batch_size,seq_len,pf_dim]\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # x : [batch_size,seq_len,hid_dim]\n",
        "    return x"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE3EQX0dyNns"
      },
      "source": [
        "class MultiHeadedAttentionComponent(nn.Module):\n",
        "  '''\n",
        "  Multiheaded attention Component. This implementation also supports mask. \n",
        "  The reason for mask that in Decoder, we don't want attention mechanism to get\n",
        "  important information from future tokens.\n",
        "  '''\n",
        "  def __init__(self,hid_dim, n_heads, dropout, device):\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim % n_heads == 0 # Since we split hid_dims into n_heads\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_heads = n_heads # no of heads in 'multiheaded' attention\n",
        "    self.head_dim = hid_dim//n_heads # dims of each head\n",
        "\n",
        "    # Transformation from source vector to query vector\n",
        "    self.fc_q = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    # Transformation from source vector to key vector\n",
        "    self.fc_k = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    # Transformation from source vector to value vector\n",
        "    self.fc_v = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    self.fc_o = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Used in self attention for smoother gradients\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None):\n",
        "\n",
        "    #query : [batch_size, query_len, hid_dim]\n",
        "    #key : [batch_size, key_len, hid_dim]\n",
        "    #value : [batch_size, value_len, hid_dim]\n",
        "\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # Transforming quey,key,values\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    #Q : [batch_size, query_len, hid_dim]\n",
        "    #K : [batch_size, key_len, hid_dim]\n",
        "    #V : [batch_size, value_len,hid_dim]\n",
        "\n",
        "    # Changing shapes to acocmadate n_heads information\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "    #Q : [batch_size, n_heads, query_len, head_dim]\n",
        "    #K : [batch_size, n_heads, key_len, head_dim]\n",
        "    #V : [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    # Calculating alpha\n",
        "    score = torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
        "    # score : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score.masked_fill(mask==0,-1e10)\n",
        "\n",
        "    alpha = torch.softmax(score,dim=-1)\n",
        "    # alpha : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    # Get the final self-attention  vector\n",
        "    x = torch.matmul(self.dropout(alpha),V)\n",
        "    # x : [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "    # Reshaping self attention vector to concatenate\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "    # x : [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "    x = x.view(batch_size,-1,self.hid_dim)\n",
        "    # x: [batch_size, query_len, hid_dim]\n",
        "\n",
        "    # Transforming concatenated outputs \n",
        "    x = self.fc_o(x)\n",
        "    #x : [batch_size, query_len, hid_dim] \n",
        "\n",
        "    return x, alpha"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0o8hEs2yOvl"
      },
      "source": [
        "class EncoderLayer(nn.Module):  \n",
        "  '''\n",
        "  Operations of a single layer in an Encoder. An Encoder employs multiple such layers. Each layer contains:\n",
        "  1) multihead attention, folllowed by\n",
        "  2) LayerNorm of addition of multihead attention output and input to the layer, followed by\n",
        "  3) FeedForward connections, followed by\n",
        "  4) LayerNorm of addition of FeedForward outputs and output of previous layerNorm.\n",
        "  '''\n",
        "  def __init__(self, hid_dim,n_heads,pf_dim,dropout,device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attn_layer_norm = nn. LayerNorm(hid_dim) #Layer norm after self-attention\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim) # Layer norm after FeedForward component\n",
        "\n",
        "    self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "    self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self,src,src_mask):\n",
        "    \n",
        "    # src : [batch_size, src_len, hid_dim]\n",
        "    # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    # get self-attention\n",
        "    _src, _ = self.self_attention(src,src,src,src_mask)\n",
        "\n",
        "    # LayerNorm after dropout\n",
        "    src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "    # src : [batch_size, src_len, hid_dim]\n",
        "\n",
        "    # FeedForward\n",
        "    _src = self.feed_forward(src)\n",
        "\n",
        "    # layerNorm after dropout\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "    # src: [batch_size, src_len, hid_dim]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8USmzaqwyP58"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  '''\n",
        "  Operations of a single layer in an Decoder. An Decoder employs multiple such layers. Each layer contains:\n",
        "  1) masked decoder self attention, followed by\n",
        "  2) LayerNorm of addition of previous attention output and input to the layer,, followed by\n",
        "  3) encoder self attention, followed by\n",
        "  4) LayerNorm of addition of result of encoder self attention and its input, followed by\n",
        "  5) FeedForward connections, followed by\n",
        "  6) LayerNorm of addition of Feedforward results and its input.\n",
        "  '''\n",
        "  def __init__(self,hid_dim,n_heads,pf_dim,dropout,device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "\n",
        "    # decoder self attention\n",
        "    self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "\n",
        "    # encoder attention\n",
        "    self.encoder_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "\n",
        "    # FeedForward\n",
        "    self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,trg, enc_src,trg_mask,src_mask):\n",
        "\n",
        "    #trg : [batch_size, trg_len, hid_dim]\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "    #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "    #src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    '''\n",
        "    Decoder self-attention\n",
        "    trg_mask is to force decoder to look only into past tokens and not get information from future tokens.\n",
        "    Since we apply mask before doing softmax, the final self attention vector gets no information from future tokens.\n",
        "    '''\n",
        "    _trg, _ = self.self_attention(trg,trg,trg,trg_mask)\n",
        "\n",
        "    # LayerNorm and dropout with resdiual connection\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "    # trg : [batch_size, trg_len, hid_dim]\n",
        "\n",
        "    '''\n",
        "    Encoder attention:\n",
        "    Query: trg\n",
        "    key: enc_src\n",
        "    Value : enc_src\n",
        "    Why? \n",
        "    the idea here is to extract information from encoder outputs. So we use decoder self-attention as a query to find important values from enc_src\n",
        "    and that is why we use src_mask, to avoid getting information from enc_src positions where it is equal to pad-id\n",
        "    After we get necessary infromation from encoder outputs we add them back to decoder self-attention.\n",
        "    '''\n",
        "    _trg, encoder_attn_alpha = self.encoder_attention(trg,enc_src,enc_src,src_mask)\n",
        "\n",
        "        # LayerNorm , residual connection and dropout\n",
        "    trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "    # trg : [ batch_size, trg_len, hid_dim]\n",
        "\n",
        "    # Feed Forward\n",
        "    _trg = self.feed_forward(trg)\n",
        "\n",
        "    # LayerNorm, residual connection and dropout\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    return trg, encoder_attn_alpha"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PoUKoRiyRIV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "  An encoder, creates token embeddings and position embeddings and passes them through multiple encoder layers\n",
        "  '''\n",
        "  def __init__(self,input_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length = 5000):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(input_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    # encoder layers\n",
        "    self.layers = nn.ModuleList([EncoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "\n",
        "    # src : [batch_size, src_len]\n",
        "    # src_mask : [batch_size,1,1,src_len]\n",
        "\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(src)*self.scale\n",
        "\n",
        "    # token plus position embeddings\n",
        "    src  = self.pos_embedding(tok_embeddings)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      src = layer(src,src_mask)\n",
        "    # src : [batch_size, src_len, hid_dim]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Bg3YGPySQ4"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "  An decoder, creates token embeddings and position embeddings and passes them through multiple decoder layers\n",
        "  '''\n",
        "  def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length= 5000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(output_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    # decoder layers\n",
        "    self.layers = nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    # convert decoder outputs to real outputs\n",
        "    self.fc_out = nn.Linear(hid_dim,output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src,trg_mask,src_mask):\n",
        "    \n",
        "    #trg : [batch_size, trg_len]\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "    #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "    #src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(trg)*self.scale\n",
        "\n",
        "    # token plus pos embeddings\n",
        "    trg = self.pos_embedding(tok_embeddings)\n",
        "    # trg : [batch_size, trg_len, hid_dim]\n",
        "\n",
        "    # Pass trg thorugh decoder layers\n",
        "    for layer in self.layers:\n",
        "      trg, encoder_attention = layer(trg,enc_src,trg_mask,src_mask)\n",
        "    \n",
        "    # trg : [batch_size,trg_len,hid_dim]\n",
        "    # encoder_attention :  [batch_size, n_head,trg_len, src_len]\n",
        "\n",
        "    # Convert to outputs\n",
        "    output = self.fc_out(trg)\n",
        "    # output : [batch_size, trg_len, output_dim]\n",
        "    \n",
        "    return output, encoder_attention"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXzefNw-yTer"
      },
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self,src):\n",
        "    # src : [batch_size, src_len]\n",
        "\n",
        "    # Masking pad values\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # src_mask : [batch_size,1,1,src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self,trg):\n",
        "    # trg : [batch_size, trg_len]\n",
        "\n",
        "    # Masking pad values\n",
        "    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # trg_pad_mask : [batch_size,1,1, trg_len]\n",
        "\n",
        "    # Masking future values\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len),device= self.device)).bool()\n",
        "    # trg_sub_mask : [trg_len, trg_len]\n",
        "\n",
        "    # combine both masks\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "    # trg_mask = [batch_size,1,trg_len,trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self,src,trg):\n",
        "\n",
        "    # src : [batch_size, src_len]\n",
        "    # trg : [batch_size, trg_len]\n",
        "\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    # src_mask : [ batch_size, 1,1,src_len]\n",
        "    # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    enc_src = self.encoder(src,src_mask)\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "\n",
        "    output, encoder_decoder_attention = self.decoder(trg,enc_src,trg_mask,src_mask)\n",
        "    # output : [batch_size, trg_len, output_dim]\n",
        "    # encoder_decoder_attention : [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    return output, encoder_decoder_attention"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmQttitayUp2"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 1024\n",
        "DEC_PF_DIM = 1024\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNZkpmLryVx7"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM4dSNbF3U4j",
        "outputId": "0400f484-70f3-49a8-b71f-12bb63c7f525"
      },
      "source": [
        "model.decoder.tok_embedding"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(2666, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD4oJ-ch4GQi",
        "outputId": "21464b25-c16e-4c56-a395-6bf2e6b54b1b"
      },
      "source": [
        "# load python embedding weights\n",
        "pretrained_embeddings = torch.load('python_embedding_weigts.pt')\n",
        "pretrained_embeddings"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(15018, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlA3wWgrz2TY"
      },
      "source": [
        "# Loading pretrained embeddings\n",
        "\n",
        "with open(\"SRC_stio\",\"rb\") as f:\n",
        "  stoi_weights = pickle.load(f)\n",
        "\n",
        "with torch.no_grad():\n",
        "  indexes = []\n",
        "  for i,j in enumerate(TRG.vocab.stoi):\n",
        "    if j in stoi_weights:\n",
        "      model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights[j]]\n",
        "    else:\n",
        "      model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights['<unk>']]\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuLULqqFyXDk",
        "outputId": "015a43bc-8ef1-4567-87f3-bf0d7b8f6b02"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,322,474 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tccCxPXjyYKO"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VGzrP2-yeg_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-zlXEgDyfk0"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiwOdJ9dygp0"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWMHus2iyhwz",
        "outputId": "c2668a4d-2eb2-43c3-f8e7-30d906ec181a"
      },
      "source": [
        "N_EPOCHS = 40\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'conala_plus_original_data.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 15s\n",
            "\tTrain Loss: 3.800 | Train PPL:  44.711\n",
            "\t Val. Loss: 2.664 |  Val. PPL:  14.355\n",
            "Epoch: 02 | Time: 0m 15s\n",
            "\tTrain Loss: 2.624 | Train PPL:  13.786\n",
            "\t Val. Loss: 2.263 |  Val. PPL:   9.611\n",
            "Epoch: 03 | Time: 0m 16s\n",
            "\tTrain Loss: 2.299 | Train PPL:   9.961\n",
            "\t Val. Loss: 2.077 |  Val. PPL:   7.980\n",
            "Epoch: 04 | Time: 0m 16s\n",
            "\tTrain Loss: 2.089 | Train PPL:   8.077\n",
            "\t Val. Loss: 1.925 |  Val. PPL:   6.856\n",
            "Epoch: 05 | Time: 0m 16s\n",
            "\tTrain Loss: 1.929 | Train PPL:   6.880\n",
            "\t Val. Loss: 1.849 |  Val. PPL:   6.356\n",
            "Epoch: 06 | Time: 0m 16s\n",
            "\tTrain Loss: 1.795 | Train PPL:   6.020\n",
            "\t Val. Loss: 1.780 |  Val. PPL:   5.928\n",
            "Epoch: 07 | Time: 0m 16s\n",
            "\tTrain Loss: 1.681 | Train PPL:   5.372\n",
            "\t Val. Loss: 1.722 |  Val. PPL:   5.596\n",
            "Epoch: 08 | Time: 0m 16s\n",
            "\tTrain Loss: 1.571 | Train PPL:   4.812\n",
            "\t Val. Loss: 1.702 |  Val. PPL:   5.486\n",
            "Epoch: 09 | Time: 0m 16s\n",
            "\tTrain Loss: 1.486 | Train PPL:   4.419\n",
            "\t Val. Loss: 1.669 |  Val. PPL:   5.304\n",
            "Epoch: 10 | Time: 0m 16s\n",
            "\tTrain Loss: 1.404 | Train PPL:   4.073\n",
            "\t Val. Loss: 1.622 |  Val. PPL:   5.065\n",
            "Epoch: 11 | Time: 0m 16s\n",
            "\tTrain Loss: 1.328 | Train PPL:   3.775\n",
            "\t Val. Loss: 1.613 |  Val. PPL:   5.020\n",
            "Epoch: 12 | Time: 0m 16s\n",
            "\tTrain Loss: 1.264 | Train PPL:   3.541\n",
            "\t Val. Loss: 1.581 |  Val. PPL:   4.860\n",
            "Epoch: 13 | Time: 0m 16s\n",
            "\tTrain Loss: 1.203 | Train PPL:   3.331\n",
            "\t Val. Loss: 1.568 |  Val. PPL:   4.798\n",
            "Epoch: 14 | Time: 0m 16s\n",
            "\tTrain Loss: 1.145 | Train PPL:   3.143\n",
            "\t Val. Loss: 1.574 |  Val. PPL:   4.826\n",
            "Epoch: 15 | Time: 0m 16s\n",
            "\tTrain Loss: 1.094 | Train PPL:   2.985\n",
            "\t Val. Loss: 1.560 |  Val. PPL:   4.761\n",
            "Epoch: 16 | Time: 0m 16s\n",
            "\tTrain Loss: 1.048 | Train PPL:   2.851\n",
            "\t Val. Loss: 1.538 |  Val. PPL:   4.657\n",
            "Epoch: 17 | Time: 0m 16s\n",
            "\tTrain Loss: 1.002 | Train PPL:   2.725\n",
            "\t Val. Loss: 1.529 |  Val. PPL:   4.611\n",
            "Epoch: 18 | Time: 0m 16s\n",
            "\tTrain Loss: 0.959 | Train PPL:   2.608\n",
            "\t Val. Loss: 1.578 |  Val. PPL:   4.844\n",
            "Epoch: 19 | Time: 0m 16s\n",
            "\tTrain Loss: 0.921 | Train PPL:   2.512\n",
            "\t Val. Loss: 1.564 |  Val. PPL:   4.776\n",
            "Epoch: 20 | Time: 0m 16s\n",
            "\tTrain Loss: 0.887 | Train PPL:   2.428\n",
            "\t Val. Loss: 1.577 |  Val. PPL:   4.841\n",
            "Epoch: 21 | Time: 0m 16s\n",
            "\tTrain Loss: 0.857 | Train PPL:   2.357\n",
            "\t Val. Loss: 1.568 |  Val. PPL:   4.798\n",
            "Epoch: 22 | Time: 0m 16s\n",
            "\tTrain Loss: 0.825 | Train PPL:   2.281\n",
            "\t Val. Loss: 1.567 |  Val. PPL:   4.792\n",
            "Epoch: 23 | Time: 0m 16s\n",
            "\tTrain Loss: 0.794 | Train PPL:   2.212\n",
            "\t Val. Loss: 1.578 |  Val. PPL:   4.846\n",
            "Epoch: 24 | Time: 0m 16s\n",
            "\tTrain Loss: 0.769 | Train PPL:   2.158\n",
            "\t Val. Loss: 1.586 |  Val. PPL:   4.886\n",
            "Epoch: 25 | Time: 0m 16s\n",
            "\tTrain Loss: 0.740 | Train PPL:   2.095\n",
            "\t Val. Loss: 1.596 |  Val. PPL:   4.935\n",
            "Epoch: 26 | Time: 0m 16s\n",
            "\tTrain Loss: 0.717 | Train PPL:   2.048\n",
            "\t Val. Loss: 1.612 |  Val. PPL:   5.012\n",
            "Epoch: 27 | Time: 0m 16s\n",
            "\tTrain Loss: 0.694 | Train PPL:   2.002\n",
            "\t Val. Loss: 1.619 |  Val. PPL:   5.046\n",
            "Epoch: 28 | Time: 0m 16s\n",
            "\tTrain Loss: 0.673 | Train PPL:   1.959\n",
            "\t Val. Loss: 1.624 |  Val. PPL:   5.073\n",
            "Epoch: 29 | Time: 0m 16s\n",
            "\tTrain Loss: 0.656 | Train PPL:   1.927\n",
            "\t Val. Loss: 1.636 |  Val. PPL:   5.134\n",
            "Epoch: 30 | Time: 0m 16s\n",
            "\tTrain Loss: 0.635 | Train PPL:   1.887\n",
            "\t Val. Loss: 1.651 |  Val. PPL:   5.211\n",
            "Epoch: 31 | Time: 0m 16s\n",
            "\tTrain Loss: 0.616 | Train PPL:   1.851\n",
            "\t Val. Loss: 1.670 |  Val. PPL:   5.314\n",
            "Epoch: 32 | Time: 0m 16s\n",
            "\tTrain Loss: 0.594 | Train PPL:   1.811\n",
            "\t Val. Loss: 1.674 |  Val. PPL:   5.332\n",
            "Epoch: 33 | Time: 0m 16s\n",
            "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
            "\t Val. Loss: 1.680 |  Val. PPL:   5.363\n",
            "Epoch: 34 | Time: 0m 16s\n",
            "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
            "\t Val. Loss: 1.713 |  Val. PPL:   5.548\n",
            "Epoch: 35 | Time: 0m 16s\n",
            "\tTrain Loss: 0.550 | Train PPL:   1.733\n",
            "\t Val. Loss: 1.711 |  Val. PPL:   5.533\n",
            "Epoch: 36 | Time: 0m 16s\n",
            "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
            "\t Val. Loss: 1.733 |  Val. PPL:   5.656\n",
            "Epoch: 37 | Time: 0m 16s\n",
            "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
            "\t Val. Loss: 1.777 |  Val. PPL:   5.911\n",
            "Epoch: 38 | Time: 0m 16s\n",
            "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
            "\t Val. Loss: 1.745 |  Val. PPL:   5.723\n",
            "Epoch: 39 | Time: 0m 16s\n",
            "\tTrain Loss: 0.498 | Train PPL:   1.645\n",
            "\t Val. Loss: 1.784 |  Val. PPL:   5.955\n",
            "Epoch: 40 | Time: 0m 16s\n",
            "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
            "\t Val. Loss: 1.770 |  Val. PPL:   5.871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvjHMGS2ykod",
        "outputId": "7e7a1501-595e-4152-9bb2-132a40ebf7f8"
      },
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"SRC_stio_local\",\"rb\") as f:\n",
        "  stoi = pickle.load(f)\n",
        "with open(\"TRG_itos_local\",\"rb\") as f:\n",
        "  itos = pickle.load(f)\n",
        "\n",
        "# Load model\n",
        "trained_model = 'conala_plus_original_data.pt'\n",
        "model.load_state_dict(torch.load(trained_model));\n",
        "model.eval()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(1661, 256)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(2666, 256)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=2666, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0q23Cs8yo8f"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def encode_inputs(input,vocab):\n",
        "\n",
        "  tokenized_input = [tok.text.lower() for tok in spacy_en.tokenizer(input)]\n",
        "  tokenized_input = ['<sos>'] + tokenized_input +['<eos>']\n",
        "\n",
        "  numericalized_input = [vocab[i] for i in tokenized_input]\n",
        "\n",
        "  tensor_input = torch.LongTensor([numericalized_input])\n",
        "  \n",
        "  return tensor_input\n",
        "\n",
        "def decode_outputs(output,vocab):\n",
        "  # output: [1,1,hid_dim]\n",
        "  predicted_token = output.argmax(-1)\n",
        "  return vocab[predicted_token.item()], predicted_token"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vXUPbhHyqBV",
        "outputId": "389332b4-cbad-4007-bb45-2c9c35e1a892"
      },
      "source": [
        " print(\" Enter q or quit to exit.\")\n",
        "\n",
        "answer_max_len = 200\n",
        "\n",
        "while(True):\n",
        "\n",
        "  input_ = input(\"Enter text:\")\n",
        "\n",
        "  if input_=='q' or input_=='quit':\n",
        "    break\n",
        "\n",
        "  src = encode_inputs(input_,stoi).to(device)\n",
        "  # src_mask = torch.ones([1,1,1,src.shape[-1]]).to(device)\n",
        "  src_mask = model.make_src_mask(src)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_src = model.encoder(src,src_mask)\n",
        "  \n",
        "  trg = '<sos>'\n",
        "  trg_indexes = [stoi[trg]]\n",
        "  # trg_mask = torch.ones([1,1,1,1]).to(device)\n",
        "\n",
        "  decoder_outputs = []\n",
        "  for i in range(answer_max_len):\n",
        "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "    trg_mask = model.make_trg_mask(trg_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      decoder_output,_ = model.decoder(trg_tensor,enc_src,trg_mask,src_mask)\n",
        "\n",
        "    pred_token = decoder_output.argmax(2)[:,-1].item()\n",
        "\n",
        "    if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
        "      break\n",
        "    decoder_outputs.append(itos[pred_token])\n",
        "    trg_indexes.append(pred_token)\n",
        "\n",
        "    \n",
        "  print(\" \".join(decoder_outputs))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Enter q or quit to exit.\n",
            "Enter text:write a program to print even length words in a string\n",
            "sentence = ' t h e   q u i c k   b r o w n   f o x ' \n",
            " words = sentence . split ( '   ' ) \n",
            " words . split ( ) \n",
            " print ( '   ' . join ( words ) ) \n",
            "Enter text:write a program to accept the strings which contains all vowels\n",
            "vowels = ' a e i o u ' \n",
            " ip_str = ' a e i o u ' \n",
            " ip_str = ip_str . casefold ( ) \n",
            " count = { } . fromkeys ( vowels , ' a ' a ' ) \n",
            " count = { } . fromkeys ( vowels , 0 ) \n",
            " for char in ip_str : \n",
            " \t if char in count : \n",
            " \t\t count [ char ] += 1 \n",
            "   print ( count ) \n",
            "Enter text:q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}